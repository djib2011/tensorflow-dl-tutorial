{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07_tensorflow_advanced.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOoT0+EmdcGaYA5QKiWrIb3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"c9D3VVJzDEiX","colab_type":"text"},"source":["# A deeper look into TensorFlow training\n","\n","This tutorial will focus on some advanced functions in TensorFlow. These allow for a more fine-grained control of training ops and significant training boosts."]},{"cell_type":"code","metadata":{"id":"wrehmYwk8khy","colab_type":"code","outputId":"e24ca149-1052-4840-85c7-8d50c88f1bb7","executionInfo":{"status":"ok","timestamp":1584897444555,"user_tz":-120,"elapsed":5688,"user":{"displayName":"Thanos Tagaris","photoUrl":"","userId":"11094556072874949144"}},"colab":{"base_uri":"https://localhost:8080/","height":632}},"source":["!pip install --upgrade tensorflow\n","import tensorflow as tf\n","assert tf.__version__[0] == '2', 'this tutorial is for tensorflow versions of 2 or higher'\n","\n","import numpy as np\n","import time"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.1.0)\n","Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n","Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.0)\n","Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n","Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n","Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n","Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.2)\n","Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0)\n","Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n","Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.24.3)\n","Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n","Requirement already satisfied, skipping upgrade: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.1)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (46.0.0)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n","Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n","Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n","Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n","Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n","Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n","Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n","Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n","Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n","Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n","Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n","Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_w7eMIoVIoeV","colab_type":"text"},"source":["## Computational Graph\n","\n","If we have a sequence of operations, we can represent these as nodes in a graph. In this representation, the edges would be the tensors that flow from operation to operation.\n","\n","For example, a fully connected layer that performs the operation:\n","\n","$$\n","z = f(W \\cdot X + b)\n","$$\n","\n","This series of operations would be represented as a computational graph as follows.\n","\n","xxxxxxxxxxxx\n","\n","It wouldn't be hard to imagine how a whole Neural Network, along with its loss function would be represented in this fashion. Now we can picture an alternate training process, where we would first define the full computation graph and then run the actual computations.\n","\n","An analogous to this would be a regular function:\n","\n","```python\n","# 1) Eagerly compute the result:\n","z = f(w * x + b)\n","\n","# 2) Lazily compute the result:\n","def fc(x):\n","  return f(w * x + b)\n","\n","# ... \n","# at this point we have defined which operations\n","# we want to run, i.e. the \"computation graph\"\n","# ...\n","\n","z = fc(x)  # run the actual computation\n","```\n","\n","This computational model has several advantages over eagerly executing the operations.\n","\n","- Computation graphs can be **simplified**. E.g. the operation `c = a + b - b` could be simplified into `c = a` even before we receive the actual values of `a` and `b`.\n","\n","- We can add to each node two operation one for the **regular** path and one for the **reverse** path. This way our computation graph can be bidirectional. This is very useful for **backpropagation**, where we want the input tensors to flow in the forward path and the gradients in the backward path.\n","\n","- We don't need to know the exact shape of the input when creating the graph. In practice this translates to **not** needing to specify the **batch size** when defining the model's architecture.\n","\n","- There are several **optimization** tricks that TensorFlow can make to computation graphs, e.g. parallel processing.\n","\n","It has its disadvantages also:\n","\n","- **Complexity**. Imagine if you had to define a function every time you needed to do a simple addition in python!\n","\n","- **Error Handling**. If we make a mistake when defining the graph, we'll know much later when actually running the graph.\n","\n","These disadvantages have led to eager execution being the default view of TensorFlow in version 2. However, in order to tap into the potential of this framework we'll need to have a basic understanding of how this works.\n","\n","Luckily, TensorFlow provides an awesome tool to make our life easier: [AutoGraph](https://www.tensorflow.org/api_docs/python/tf/autograph). This is involked through the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) decorator and converts any regular python syntax into a TensorFlow computation graph!"]},{"cell_type":"code","metadata":{"id":"IXXLgi-CzvFt","colab_type":"code","outputId":"992be135-1115-4f33-864b-c0dc73f71833","executionInfo":{"status":"ok","timestamp":1584897445709,"user_tz":-120,"elapsed":6825,"user":{"displayName":"Thanos Tagaris","photoUrl":"","userId":"11094556072874949144"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Regular keras FC layer\n","fc = tf.keras.layers.Dense(100)\n","\n","# Same operation converted to a TensorFlow graph\n","@tf.function\n","def fc_graph(x):\n","  return fc(x)\n","\n","# Create dummy data\n","X = tf.random.normal(shape=(1000, 100))\n","\n","# Run each operation once for warmup\n","# (we have an initial \"cost\" of having to construct the graph, to evaluate which\n","# of the two is faster we need to take this out of the equation.)\n","fc(X); fc_graph(X)\n","\n","# Run a forward pass on both keras and our tf-wrapped layer\n","t1 = time.time()\n","z1 = fc(X)\n","t2 = time.time()\n","z2 = fc_graph(X)\n","t3 = time.time()\n","\n","# Assert that we performed the same operation and we got the same result\n","print('Same result:', np.array_equal(z1, z2))\n","\n","# Print the results\n","print('Eager time: {:.2f}ms'.format((t2 - t1) * 1000))\n","print('Graph time: {:.2f}ms'.format((t3 - t2) * 1000))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Same result: True\n","Eager time: 0.41ms\n","Graph time: 0.37ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5qOPq39QYzkc","colab_type":"text"},"source":["This difference magnifies in other types of layers. The following example was taken from the [official TensorFlow guide on tf.functions](https://www.tensorflow.org/guide/function#the_tffunction_decorator),"]},{"cell_type":"code","metadata":{"id":"O-UyINNL0TBj","colab_type":"code","outputId":"8cad8bee-1216-44dc-993f-023f148c7559","executionInfo":{"status":"ok","timestamp":1584897445711,"user_tz":-120,"elapsed":6813,"user":{"displayName":"Thanos Tagaris","photoUrl":"","userId":"11094556072874949144"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Keras LSTM layer\n","lstm_cell = tf.keras.layers.LSTMCell(10)\n","\n","# Convert to tf Graph\n","@tf.function\n","def lstm_fn(input, state):\n","  return lstm_cell(input, state)\n","\n","# Generate \"dummy\" input data\n","inp = tf.zeros([10, 10])\n","state = [tf.zeros([10, 10])] * 2\n","\n","# Warmup\n","lstm_cell(inp, state); lstm_fn(inp, state)\n","\n","# Run the benchmark\n","t1 = time.time()\n","z1 = lstm_cell(inp, state)\n","t2 = time.time()\n","z2 = lstm_fn(inp, state)\n","t3 = time.time()\n","\n","print('Same result:', np.array_equal(z1, z2))\n","\n","print('Eager time: {:.2f}ms'.format((t2 - t1) * 1000))\n","print('Graph time: {:.2f}ms'.format((t3 - t2) * 1000))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Same result: False\n","Eager time: 0.83ms\n","Graph time: 0.49ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F1aBHUyxXtuS","colab_type":"text"},"source":["Here the forward pass takes up almost **half the time** when run in graph mode!\n","\n","Some other things to note about the `tf.function` wrapper are that:\n","\n","- If we have a nested structure we just need to apply the decorator to the outer function. It will work on its own in the inner function as well. For example:\n","\n","```python\n","# no need for @tf.function here\n","def a(x):\n","    return 2 * x\n","\n","@tf.function\n","def b(x):\n","    return 3 * a(x)\n","```\n","\n","- AutoGraph inhenetly handles any python control structures by converting the native python ops to TensorFlow graph ops. For example, python's `while` will become [`tf.while_loop`](https://www.tensorflow.org/api_docs/python/tf/while_loop), `if` will become [`tf.cond`](https://www.tensorflow.org/api_docs/python/tf/cond), etc.\n","\n","- AutoGraph is enabled **by default** for all non-dynamic keras models.\n","\n","- The first time a `tf.function` decorated function is involked, the AutoGraph module is called to construct the graph. This procedure is called [tracing](https://www.tensorflow.org/guide/function) and takes time. For this reason reason avoid applying `tf.function` to low-level local function; rather try to decorate module-level functions and class methods.  *Note: this is the reason we used a \"warmup\" when timing the execution before.*  A more in-depth analysis of this can be found [here](https://www.tensorflow.org/guide/function#re-tracing).\n","\n","- Graphs were the \"default view\" of TensorFlow in versions `< 2.0`. TensorFlow still allows for the [direct (i.e. manual) construction of a graph](https://www.tensorflow.org/api_docs/python/tf/Graph#using_graphs_directly_deprecated). However, this approach is **deprecated** and should be avoided.\n","\n","- Asynchronous batch prefetching (i.e. the `tf.Dataset` feature we saw in the previous tutorial, where batches were prepared while the model was training) is [only available](https://www.tensorflow.org/guide/effective_tf2#combine_tfdatadatasets_and_tffunction) when running in graph mode.\n","\n","- Several **limitations** of AutoGraph (along with their workarounds) can be found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md). It is worthwile to go through them if you're planning to use the `tf.function` decorator.\n","\n","- TensorFlow uses a tool called [Grappler] for **optimizing** its graphs. These optimizations include *pruning* redundant nodes, *stripping* debugging operations, *low-level memory mapping*, *parallelization* and many more. More info about these can be found [here](https://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf). Most of these are **enabled by default**, however Grappler also allows for a more [fine-grained control](https://www.tensorflow.org/api_docs/python/tf/config/optimizer/set_experimental_options).  \n","\n","## Custom Training Loop\n","\n","In the previous tutorials we saw how we can easily train keras models through the `.fit()` and `.fit_generator()` methods. One feature of these methods is that they intentionally **hide the training loop**.\n","\n","What would normally be\n","\n","```python\n","for e in range(epochs):\n","  for x, y in zip(x_train, y_train):\n","    # ...\n","    # train on batch\n","    # ...\n","```\n","\n","becomes\n","\n","```python\n","model.fit(x_train, y_train, epochs=epochs)\n","```\n","\n","This is done for the sake of simplicity, but in the expense of **control**. To affect some aspects of the training process (e.g. learning rate) or to monitor variables, we need to use one of the existing or write a custom callback.\n","\n","Still there are limitations to what we can affect or monitor through the use of callbacks! For this reason, in some cases we might want to train the model in a custom loop, where we have full control.\n","\n","The fundamental steps we need to perform **at each iteration** are:\n","\n","1. Generate the training **batch** (samples + labels).\n","2. Perform the **forward pass** and get the model's **predictions** for this batch.\n","3. Calculate the **loss** of these predictions compared to the actual labels.\n","4. Compute the loss' **gradients** w.r.t the model's trainable parameters (i.e. **backprop**).\n","5. **Update** the model's parameters according to the gradients (i.e. **optimizer**).\n","\n","Let's try by defining our dataset, like we saw in the precious tutorial. This time we'll use the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, which consists of $60000$, $32 \\times 32$, RGB images, evenly distributed into $10$ classes."]},{"cell_type":"code","metadata":{"id":"3rgxPyaNdNDs","colab_type":"code","colab":{}},"source":["def preprocess_data(x, y):\n","  '''\n","  Preprocess a single image-label pair. \n","  '''\n","  x = tf.image.convert_image_dtype(x, tf.float32)\n","  y = tf.one_hot(tf.cast(tf.squeeze(y), tf.int32), 10)\n","  return x, y\n","\n","def generate_cifar():\n","  '''\n","  Generate the train and test set datasets for CIFAR-10.\n","  '''\n","  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","\n","  train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n","\n","  train = train.map(preprocess_data)\n","  test = test.map(preprocess_data)\n","  \n","  train = train.shuffle(50000)\n","  test = test.shuffle(10000)\n","\n","  train = train.repeat()\n","  test = test.repeat()\n","  \n","  train = train.batch(256)\n","  test = test.batch(256)\n","  \n","  train.prefetch(1)\n","  test.prefetch(1)\n","\n","  return train, test\n","\n","train_set, test_set = generate_cifar()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xmQhEBHV635a","colab_type":"text"},"source":["Now, let's build a simple 4-layer keras CNN."]},{"cell_type":"code","metadata":{"id":"HqcY_JG59Rfm","colab_type":"code","outputId":"76406a0d-82ff-42ac-da52-84cf14b00f2f","executionInfo":{"status":"ok","timestamp":1584897446956,"user_tz":-120,"elapsed":8022,"user":{"displayName":"Thanos Tagaris","photoUrl":"","userId":"11094556072874949144"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["def make_cnn(input_shape=(32, 32, 3)):\n","  inp = tf.keras.layers.Input(input_shape)\n","  c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(inp)\n","  c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(c1)\n","  c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu')(c2)\n","  fl = tf.keras.layers.Flatten()(c3)\n","  out = tf.keras.layers.Dense(10, activation='softmax')(fl)\n","\n","  model = tf.keras.models.Model(inp, out)\n","\n","  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  return model\n","\n","model = make_cnn()\n","\n","model.summary()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n","_________________________________________________________________\n","conv2d (Conv2D)              (None, 30, 30, 32)        896       \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 28, 28, 64)        18496     \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 26, 26, 128)       73856     \n","_________________________________________________________________\n","flatten (Flatten)            (None, 86528)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                865290    \n","=================================================================\n","Total params: 958,538\n","Trainable params: 958,538\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NM1h6bG3L1NY","colab_type":"text"},"source":["Let train the model through keras' `.fit()` API as a baseline."]},{"cell_type":"code","metadata":{"id":"xL6pTD3fDYwn","colab_type":"code","outputId":"cf771c53-474a-4cb4-b46b-e776d2177403","executionInfo":{"status":"ok","timestamp":1584897511839,"user_tz":-120,"elapsed":72888,"user":{"displayName":"Thanos Tagaris","photoUrl":"","userId":"11094556072874949144"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["start_time = time.time()\n","model.fit(train_set, epochs=5, steps_per_epoch=(50000//128), \n","          validation_data=test_set, validation_steps=(10000//128))\n","print('Time elapsed: {:.2f}'.format(time.time() - start_time))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Train for 390 steps, validate for 78 steps\n","Epoch 1/5\n","390/390 [==============================] - 15s 40ms/step - loss: 1.3692 - accuracy: 0.5156 - val_loss: 1.1192 - val_accuracy: 0.6118\n","Epoch 2/5\n","390/390 [==============================] - 12s 31ms/step - loss: 0.9193 - accuracy: 0.6815 - val_loss: 0.9737 - val_accuracy: 0.6612\n","Epoch 3/5\n","390/390 [==============================] - 12s 31ms/step - loss: 0.7087 - accuracy: 0.7574 - val_loss: 0.9803 - val_accuracy: 0.6711\n","Epoch 4/5\n","390/390 [==============================] - 12s 30ms/step - loss: 0.5266 - accuracy: 0.8214 - val_loss: 1.0447 - val_accuracy: 0.6781\n","Epoch 5/5\n","390/390 [==============================] - 13s 34ms/step - loss: 0.3586 - accuracy: 0.8790 - val_loss: 1.2481 - val_accuracy: 0.6590\n","Time elapsed: 64.77\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CywnuhocMTDs","colab_type":"text"},"source":["Now let's create a custom training loop to do the same thing.\n","\n","For simplicity we'll first create a function that performs a single training step on the data."]},{"cell_type":"code","metadata":{"id":"eaOAfpnpMrPc","colab_type":"code","colab":{}},"source":["# Define the same loss function, the optimizer and the metric we used previsouly\n","loss_function = tf.keras.losses.categorical_crossentropy\n","optimizer = tf.keras.optimizers.Adam()\n","accuracy = tf.keras.metrics.Accuracy()\n","\n","\n","# We'll now define a function that performs a single training step. We don't\n","# need to apply the tf.function decorator here, because this function isn't \n","# intended to be a top-level function (i.e. it will be called from another).\n","def train_on_batch(x, y):\n","  '''\n","  Will train 'model' as defined in the global scope for a single training batch.\n","  The optimizer and loss function will also be taken from the global scope.\n","  '''\n","  # Start monitoring the operations in order to compute the gradient:\n","  with tf.GradientTape() as tape:\n","  \n","    # Generate the model's prediction:\n","    y_hat = model(x)\n","\n","    # Calculate its loss:\n","    loss = loss_function(y, y_hat)\n","\n","  # Compute the gradient of the loss w.r.t the model's parameters\n","  grads = tape.gradient(loss, model.trainable_variables)\n","\n","  # Update the model's parameters through the optimizer\n","  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","  # Store the batch's accuracy\n","  acc = accuracy(tf.argmax(y, axis=1), tf.argmax(y_hat, axis=1))\n","\n","  # Get the batch's loss so that we can plot it later\n","  return tf.reduce_mean(loss), tf.reduce_mean(acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivmADbr3Upxw","colab_type":"text"},"source":["Now let's write a function that uses the previous to train the model for a number of epochs."]},{"cell_type":"code","metadata":{"id":"0piZ8p7armyw","colab_type":"code","colab":{}},"source":["train_loss = tf.keras.metrics.Mean()\n","train_acc = tf.keras.metrics.Mean()\n","valid_acc = tf.keras.metrics.Mean()\n","\n","@tf.function\n","def train(train_data, epochs, validation_data=None):\n","  '''\n","  Train 'model' as defined in the global scope for a number of epochs, on a \n","  given dataset. \n","  '''\n","\n","  for e in range(epochs):\n","\n","    tf.print('Epoch {} of {}'.format(e+1, epochs))\n","    \n","    i = 0\n","    loss = 0.\n","    acc = 0.\n","\n","    # Training epoch:\n","    for x, y in train_data:\n","\n","      i += 1\n","\n","      # Perform a single iteration\n","      loss, acc = train_on_batch(x, y)\n","\n","      # We need to manually keep track of the iteration,\n","      # because this generator will loop forever \n","      if i >= 50000 // 128 + 1:\n","        break\n","\n","      if i % 100 == 0:\n","        tf.print('    Iteration:', i)\n","        tf.print('        Training loss:', loss)\n","        tf.print('        Training acc:', acc)\n","\n","    if validation_data:\n","\n","      i = 0\n","      valid_acc = 0.\n","\n","      # Iterate over the validation set:\n","      for x, y in validation_data:\n","\n","        i += 1\n","\n","        # Make a prediction and compute the mean of the accuracy and the loss\n","        preds = model(x)\n","        \n","        # Compute the mean accuracy of each batch and add them to 'valid_acc'\n","        valid_acc += accuracy(tf.argmax(y, axis=1), tf.argmax(preds, axis=1))\n","        \n","        # Again keep track of the iteration in order to terminate the loop\n","        if i >= 10000 // 128 + 1:\n","          break\n","\n","      tf.print('    Validation accuracy:', valid_acc / tf.cast(i, tf.float32))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S1PIiI62ckwa","colab_type":"text"},"source":["We can finally train our model now."]},{"cell_type":"code","metadata":{"id":"9uH-y0bvcqe7","colab_type":"code","outputId":"e75cd284-fbe9-46bb-b21d-0e548c5ed4a8","executionInfo":{"status":"ok","timestamp":1584897598274,"user_tz":-120,"elapsed":159285,"user":{"displayName":"Thanos Tagaris","photoUrl":"","userId":"11094556072874949144"}},"colab":{"base_uri":"https://localhost:8080/","height":969}},"source":["model = make_cnn()\n","start_time = time.time()\n","train(train_set, epochs=5, validation_data=test_set)\n","print('Time elapsed: {:.2f}sec'.format(time.time() - start_time))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Epoch 1 of 5\n","    Iteration: 100\n","        Training loss: 1.33137321\n","        Training acc: 0.398554683\n","    Iteration: 200\n","        Training loss: 1.29405034\n","        Training acc: 0.469453126\n","    Iteration: 300\n","        Training loss: 1.08646286\n","        Training acc: 0.516992211\n","    Validation accuracy: 0.554237247\n","Epoch 2 of 5\n","    Iteration: 100\n","        Training loss: 0.895894587\n","        Training acc: 0.582689166\n","    Iteration: 200\n","        Training loss: 0.765978038\n","        Training acc: 0.599661827\n","    Iteration: 300\n","        Training loss: 0.777137578\n","        Training acc: 0.616878033\n","    Validation accuracy: 0.631168425\n","Epoch 3 of 5\n","    Iteration: 100\n","        Training loss: 0.60319674\n","        Training acc: 0.64668721\n","    Iteration: 200\n","        Training loss: 0.44415462\n","        Training acc: 0.657404721\n","    Iteration: 300\n","        Training loss: 0.566433847\n","        Training acc: 0.670649588\n","    Validation accuracy: 0.679828286\n","Epoch 4 of 5\n","    Iteration: 100\n","        Training loss: 0.421858311\n","        Training acc: 0.692094386\n","    Iteration: 200\n","        Training loss: 0.280226111\n","        Training acc: 0.701652288\n","    Iteration: 300\n","        Training loss: 0.313862085\n","        Training acc: 0.713096201\n","    Validation accuracy: 0.721002221\n","Epoch 5 of 5\n","    Iteration: 100\n","        Training loss: 0.202499\n","        Training acc: 0.731001437\n","    Iteration: 200\n","        Training loss: 0.11393778\n","        Training acc: 0.740065336\n","    Iteration: 300\n","        Training loss: 0.229704469\n","        Training acc: 0.750143349\n","    Validation accuracy: 0.756498039\n","Time elapsed: 86.24sec\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NZ-0IGzsZ63W","colab_type":"text"},"source":["In the next tutorial we'll see a visualization tool TensorFlow offers to debug our model's graph and inspect its performance during training."]}]}